{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization (Senetnce Ranking)\n",
    "Metode sentence ranking digunakan untuk membuat ringkasan dokumen dengan memilih kalimat-kalimat paling penting berdasarkan tingkat keterkaitannya dengan kalimat lain dalam dokumen.\n",
    "\n",
    "### Tahapan Prosesnya yaitu:\n",
    "#### 1. Data cleaning ( removing non letter characters, turning to lower case letters ):\n",
    "        Membersihkan teks dengan menghapus karakter non-huruf, tanda baca, dan mengubah seluruh teks menjadi huruf kecil agar mudah diproses.\n",
    "#### 2. Building Sentence Similarity Matrix: \n",
    "        Menghitung tingkat kemiripan antar kalimat menggunakan representasi vektor (misalnya TF-IDF atau cosine similarity) dan menyimpannya dalam bentuk matriks.\n",
    "#### 3. Sentence Ranking: \n",
    "        Memberi skor pada setiap kalimat berdasarkan tingkat kepentingannya, biasanya menggunakan algoritma berbasis graf seperti PageRank.\n",
    "#### 4. Summary Generation: \n",
    "        Memilih beberapa kalimat dengan skor tertinggi sebagai ringkasan dokumen tanpa mengubah struktur kalimat aslinya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Phase\n",
    "### Importing Libraries and Reading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: networkx in c:\\users\\mmurt\\appdata\\roaming\\python\\python312\\site-packages (3.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing the necessary libraries\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from nltk.tokenize import  sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Maria Sharapova has basically no friends as te...\n",
       "1    BASEL, Switzerland (AP), Roger Federer advance...\n",
       "2    Roger Federer has revealed that organisers of ...\n",
       "3    Kei Nishikori will try to end his long losing ...\n",
       "4    Federer, 37, first broke through on tour over ...\n",
       "5    Nadal has not played tennis since he was force...\n",
       "6    Tennis giveth, and tennis taketh away. The end...\n",
       "7    Federer won the Swiss Indoors last week by bea...\n",
       "Name: article_text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading the data file\n",
    "\n",
    "df = pd.read_csv('../dataset/tennis_articles_v4.csv')\n",
    "df['article_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## working with re ( regular expression in python)\n",
    "\n",
    "import re\n",
    "s = 'he&&&s'\n",
    "s = re.sub(\"[^a-zA-Z]\",\" \",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : Data Cleaning\n",
    "Membersihkan Kalimat dengan Menghapus Karakter Non-Huruf dan Mengubah ke Huruf Kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning sentences, by removing non alphabet characters and converting to lower case letters\n",
    "\n",
    "dict = {}\n",
    "s = \"\"\n",
    "for a in df['article_text']:\n",
    "      s += a\n",
    "# print s\n",
    "s = s.lower()\n",
    "# print s\n",
    "\n",
    "sentences = sent_tokenize(s)\n",
    "# print sentences\n",
    "\n",
    "final = []\n",
    "\n",
    "for s in sentences:\n",
    "      temp = re.sub(\"[^a-zA-Z]\",\" \",s)\n",
    "      temp = temp.lower()\n",
    "      final.append(temp)\n",
    "      dict[temp] = s\n",
    "# printfinal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : Building Senetnce Similarity Matrix\n",
    "Menghitung Kemiripan Menggunakan Cosine Similarity antar Representasi Vektor Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define method for calculating similarity\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : Sentence Ranking\n",
    "Mengurutkan Kalimat Menggunakan Algoritma PageRank pada Graf Kemiripan Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generating the final summary : graph is generated using networkx library and cosine similarity matrix \n",
    "# containg adjacency list; after that sentences are scored using pagerank and sorted and stored in ranked_sentences\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "sentence_similarity_martix = build_similarity_matrix(final, '')\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(final)), reverse=True)    \n",
    "# print type(ranked_sentence)\n",
    "# print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "\n",
    "\n",
    "# Step 5 - Offcourse, output the summarize texr\n",
    "# print('Summarize Text: \\n', \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 : Summary Generation\n",
    "Menampilkan 10 Kalimat dengan Peringkat Tertinggi sebagai Ringkasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argentina and britain received wild cards to the new-look event, and will compete along with the four 2018 semi-finalists and the 12 teams who win qualifying rounds next february.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "     print(dict[ranked_sentence[i][1]])\n",
    "        \n",
    "# for i in range(10):\n",
    "#       summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
